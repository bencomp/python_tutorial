{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data acquisition - Solutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10.1.\n",
    "\n",
    "The list below contains a number of URLs. They are the web addresses of texts created for the [Project Gutenberg](https://www.gutenberg.org) website.\n",
    "\n",
    "```\n",
    "urls = [ 'https://www.gutenberg.org/files/580/580-0.txt' ,\n",
    "'https://www.gutenberg.org/files/1400/1400-0.txt' ,\n",
    "'https://www.gutenberg.org/files/786/786-0.txt' ,\n",
    "'https://www.gutenberg.org/files/766/766-0.txt' \n",
    "]\n",
    "```\n",
    "\n",
    "Write a program in Python that downloads all the files in this list and stores them in the current directory.\n",
    "As filenames, use the same names that are used by Project Gutenberg (e.g. '580-0.txt' or '1400-0.txt').\n",
    "The basename in a URL can be extracted using the [`os.path.basename()`](https://docs.python.org/3/library/os.path.html#os.path.basename) function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os \n",
    "\n",
    "# Recreate the given list using copy and paste\n",
    "urls = [ 'https://www.gutenberg.org/files/580/580-0.txt' ,\n",
    "'https://www.gutenberg.org/files/1400/1400-0.txt' ,\n",
    "'https://www.gutenberg.org/files/786/786-0.txt' ,\n",
    "'https://www.gutenberg.org/files/766/766-0.txt' \n",
    "]\n",
    "\n",
    "# We use a for-loop to take the same steps for each item in the list:\n",
    "for url in urls:\n",
    "    # 1. Download the file contents\n",
    "    response = requests.get(url)\n",
    "    # 1a. Force the textual contents to be interpreted as UTF-8 encoded, because the website does not send the text encoding\n",
    "    response.encoding = 'utf-8'\n",
    "    # 2. Use basename to get a suitable filename\n",
    "    filename = os.path.basename(url)\n",
    "    # 3. Open the file in write mode and write the downloaded file contents to the file\n",
    "    out = open( filename , mode = 'w', encoding= 'utf-8' )\n",
    "    out.write( response.text )\n",
    "    # 4. Close the file\n",
    "    out.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10.2.\n",
    "\n",
    "Write Python code which can download the titles and the URLs of Wikipedia articles whose titles contain the word 'Dutch'. Your code needs to display the first 30 results only.\n",
    "\n",
    "*Hint: the tutorial covers the Wikipedia API.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Let's construct the full API call (which is a URL) piece by piece\n",
    "baseURL = 'https://en.wikipedia.org/w/api.php?action=opensearch'\n",
    "\n",
    "searchTerm = \"Dutch\"\n",
    "limit = 30\n",
    "data_format = 'json'\n",
    "\n",
    "apiCall = '{}&search={}&limit={}&format={}'.format( baseURL, searchTerm , limit , data_format )\n",
    "\n",
    "# Get the data using the Requests library\n",
    "responseData = requests.get( apiCall )\n",
    "\n",
    "# Because we asked for and got JSON-formatted data, Requests lets us access\n",
    "# the data as a Python data structure using the .json() method\n",
    "wikiResults = responseData.json()\n",
    "\n",
    "# Now we print the search results \n",
    "for i in range( 0 , len(wikiResults[1]) ):\n",
    "    print( 'Title: ' + wikiResults[1][i] )\n",
    "    print( 'Tagline: ' + wikiResults[2][i] )\n",
    "    print( 'Url: ' + wikiResults[3][i] + '\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10.3.\n",
    "\n",
    "Write an application in Python that extracts all the publications that have been added to a specific ORCID account, using the ORCID API.\n",
    "\n",
    "Information about individual ORCID accounts can be obtained by appending their ID to the base URL <https://pub.orcid.org/v2.0/>. The ORCID API returns data in XML by default. In the XML, the list of publications can be found using the XPath `r:record/a:activities-summary/a:works/a:group` (using the namespace declarations given below).\n",
    "\n",
    "*Note: we use the [ElementTree](https://docs.python.org/3/library/xml.etree.elementtree.html) library to process the XML data. It is very powerful, but has a quite steep learning curve.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orcid = '0000-0002-8469-6804'\n",
    "\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Declare namespace abbreviations\n",
    "ns = {'o': 'http://www.orcid.org/ns/orcid' ,\n",
    "'s' : 'http://www.orcid.org/ns/search' ,\n",
    "'h': 'http://www.orcid.org/ns/history' ,\n",
    "'p': 'http://www.orcid.org/ns/person' ,\n",
    "'pd': 'http://www.orcid.org/ns/personal-details' ,\n",
    "'a': 'http://www.orcid.org/ns/activities' ,\n",
    "'e': 'http://www.orcid.org/ns/employment' ,\n",
    "'c': 'http://www.orcid.org/ns/common' , \n",
    "'w': 'http://www.orcid.org/ns/work',\n",
    "'r': 'http://www.orcid.org/ns/record'}\n",
    "\n",
    "\n",
    "try:\n",
    "    # Construct the API call and retrieve the data\n",
    "    orcidUrl = \"https://pub.orcid.org/v2.0/\" + orcid\n",
    "    print( orcidUrl )\n",
    "    \n",
    "    response = requests.get( orcidUrl )\n",
    "    \n",
    "    # Parse XML string into its Python ElementTree object representation\n",
    "    root = ET.fromstring(response.text)\n",
    "    \n",
    "    # Find and print the ORCID creation date\n",
    "    creationDate = root.find('h:history/h:submission-date' , ns ).text\n",
    "    \n",
    "    print('\\nORCID created on:')\n",
    "    print(creationDate)\n",
    "    \n",
    "    # Print the title and DOI of each work (DOI only when available)\n",
    "    print('\\nWorks:')\n",
    "    \n",
    "    works = root.findall('a:activities-summary/a:works/a:group' , ns )\n",
    "    for w in works:\n",
    "        title = w.find('w:work-summary/w:title/c:title' , ns ).text\n",
    "        print(title)\n",
    "        doiEl = w.find('c:external-ids/c:external-id/c:external-id-url' , ns )\n",
    "        if doiEl is not None:\n",
    "            doi = doiEl.text\n",
    "            print(doi)\n",
    "            \n",
    "except:\n",
    "    print(\"Data could not be downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10.4.\n",
    "\n",
    "The API developed by [OpenStreetMap](https://www.openstreetmap.org/) can be used, among other things, to find the precise geographic coordinates of a specific location. The base URL of this API is https://nominatim.openstreetmap.org/search. Following the `q` parameter, you need to supply a string describing the locations whose latitude and longitude you want to find. As values for the `format` parameter, you can use `xml` for XML-formatted data or `json` for JSON-formatted data. Use this API to find the longitude and the latitude of the addresses in the following list:\n",
    "\n",
    "```\n",
    "addresses = ['Grote Looiersstraat 17 Maastricht' , 'Witte Singel 27 Leiden' ,\n",
    "'Singel 425 Amsterdam' , 'Drift 27 Utrecht' , 'Broerstraat 4 Groningen']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import string\n",
    "from os.path import isfile, join, isdir\n",
    "import os\n",
    "\n",
    "addresses = ['Grote Looiersstraat 17 Maastricht' , 'Witte Singel 27 Leiden' , 'Singel 425 Amsterdam' , 'Drift 27 Utrecht' , 'Broerstraat 4 Groningen']\n",
    "\n",
    "\n",
    "for a in addresses:\n",
    "    # Construct the API query and get the data\n",
    "    # Note that spaces are not allowed in URLs, so they need to be 'escaped', i.e. replaced by %20\n",
    "    url = 'https://nominatim.openstreetmap.org/search?q='+ a + '&format=xml'\n",
    "    url = re.sub( '\\s+' , '%20' , url )\n",
    "    # See the note below about URL escaping\n",
    "\n",
    "    print(\"Trying {}...\".format(url))\n",
    "    response = requests.get( url )\n",
    "    \n",
    "    # Parse the XML using the ElementTree library and find places\n",
    "    root = ET.fromstring( response.text )\n",
    "    places = root.findall('place')\n",
    "    \n",
    "    # Print the first result, if there are results\n",
    "    if places is not None:\n",
    "        place = places[0]\n",
    "        lat = place.attrib['lat']\n",
    "        lon = place.attrib['lon']\n",
    "        print( '{}: {},{}\\n'.format( a, lat , lon ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL escaping\n",
    "\n",
    "Replacing spaces with the escape sequence for spaces is not the same as is just one part of 'URL escaping'. There are other, preferred, ways of escaping illegal characters in URLs, like [urllib.parse.urlencode](https://docs.python.org/3/library/urllib.parse.html#urllib.parse.urlencode) specifically meant for query parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
